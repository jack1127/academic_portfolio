{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計學習與深度學習\n",
    "### Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of pictures in train= 1056\n",
      "amount of blazer in train = 99\n",
      "account for 9.38 %\n",
      "amount of cardigan in train = 243\n",
      "account for 23.01 %\n",
      "amount of coat in train = 297\n",
      "account for 28.12 %\n",
      "amount of jacket in train = 417\n",
      "account for 39.49 %\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(1223)\n",
    "labels = ['blazer', 'cardigan', 'coat', 'jacket']\n",
    "\n",
    "pic_num_train = [0, 0, 0, 0]\n",
    "pic_sum_train = 0\n",
    "\n",
    "for i in range(4):\n",
    "    basepath = os.path.join(\"photos/train/\", labels[i])\n",
    "    for root,dirs,files in os.walk(basepath):\n",
    "        for each in files:\n",
    "            pic_num_train[i] += 1\n",
    "    \n",
    "    pic_sum_train += pic_num_train[i]\n",
    "print(\"Total amount of pictures in train=\", pic_sum_train)\n",
    "for i in range(4):\n",
    "    print(\"amount of\", labels[i], \"in train =\", pic_num_train[i])\n",
    "    print(\"account for\", \"%.2f\" %(pic_num_train[i]/pic_sum_train*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of pictures in valid = 105\n",
      "amount of blazer in valid = 7\n",
      "account for 6.67 %\n",
      "amount of cardigan in valid = 36\n",
      "account for 34.29 %\n",
      "amount of coat in valid = 27\n",
      "account for 25.71 %\n",
      "amount of jacket in valid = 35\n",
      "account for 33.33 %\n"
     ]
    }
   ],
   "source": [
    "pic_num_valid = [0, 0, 0, 0]\n",
    "pic_sum_valid = 0\n",
    "for i in range(4):\n",
    "    basepath = os.path.join(\"photos/valid/\", labels[i])\n",
    "    for root,dirs,files in os.walk(basepath):\n",
    "        for each in files:\n",
    "            pic_num_valid[i] += 1\n",
    "    \n",
    "    pic_sum_valid += pic_num_valid[i]\n",
    "print(\"Total amount of pictures in valid =\", pic_sum_valid)\n",
    "for i in range(4):\n",
    "    print(\"amount of\", labels[i], \"in valid =\", pic_num_valid[i])\n",
    "    print(\"account for\", \"%.2f\" %(pic_num_valid[i]/pic_sum_valid*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of pictures in test = 146\n",
      "amount of blazer in test = 9\n",
      "account for 6.16 %\n",
      "amount of cardigan in test = 42\n",
      "account for 28.77 %\n",
      "amount of coat in test = 43\n",
      "account for 29.45 %\n",
      "amount of jacket in test = 52\n",
      "account for 35.62 %\n"
     ]
    }
   ],
   "source": [
    "pic_num_test = [0, 0, 0, 0]\n",
    "pic_sum_test = 0\n",
    "for i in range(4):\n",
    "    basepath = os.path.join(\"photos/test/\", labels[i])\n",
    "    for root,dirs,files in os.walk(basepath):\n",
    "        for each in files:\n",
    "            pic_num_test[i] += 1\n",
    "    \n",
    "    pic_sum_test += pic_num_test[i]\n",
    "print(\"Total amount of pictures in test =\", pic_sum_test)\n",
    "for i in range(4):\n",
    "    print(\"amount of\", labels[i], \"in test =\", pic_num_test[i])\n",
    "    print(\"account for\", \"%.2f\" %(pic_num_test[i]/pic_sum_test*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我覺得blazer的準確率應該會最高，且明顯較高，因為樣式比較固定且材質特殊，容易捕捉特徵。\n",
    "再來是jacket，雖然它的材質歧異度大，不易捕捉特徵，但跟剩下的兩種差異較明顯，所以準確率應該會略高於剩下兩個，位居第二。\n",
    "cardigan跟coat的相似度頗高，而cardigan的材質歧異度又較大，不易捕捉特徵，可能被判斷成jacket或coat，所以我認為cardigan準確率應該會最低。\n",
    "coat則是第三。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (順序寫錯了，Q2在後面)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "resnet50 = models.resnet50(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomRotation(degrees=30),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"photos\"\n",
    "train_basepath = os.path.join(dataset, \"train\")\n",
    "valid_basepath = os.path.join(dataset, \"valid\")\n",
    "test_basepath = os.path.join(dataset, \"test\")\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_EPOCHS = 200\n",
    "NUM_CLASS = 4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_basepath, transform=image_transforms['train']),\n",
    "    \n",
    "    'valid': datasets.ImageFolder(root=valid_basepath, transform=image_transforms['valid']),\n",
    "    \n",
    "    'test': datasets.ImageFolder(root=test_basepath, transform=image_transforms['test'])\n",
    "}\n",
    " \n",
    " \n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size = len(data['test'])\n",
    "\n",
    "train_data = DataLoader(data['train'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_data = DataLoader(data['valid'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_data = DataLoader(data['test'], batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "fc_inputs = resnet50.fc.in_features\n",
    "\n",
    "resnet50.fc = nn.Sequential(\n",
    "    nn.Linear(fc_inputs, 4),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(resnet50.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_valid(model, loss_function, optimizer, epochs):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience = 20\n",
    "    trigger_times = 0\n",
    "    last_avg_valid_loss = 100000\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    " \n",
    "        model.train()\n",
    " \n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    " \n",
    "        for i, (inputs, labels) in enumerate(train_data):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    " \n",
    "            optimizer.zero_grad()\n",
    " \n",
    "            outputs = model(inputs)\n",
    " \n",
    "            loss = loss_function(outputs, labels)\n",
    " \n",
    "            loss.backward()\n",
    " \n",
    "            optimizer.step()\n",
    " \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    " \n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    " \n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    " \n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    " \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    " \n",
    "            for j, (inputs, labels) in enumerate(valid_data):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    " \n",
    "                loss = loss_function(outputs, labels)\n",
    " \n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    " \n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    " \n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    " \n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    " \n",
    "        avg_train_loss = train_loss/train_data_size\n",
    "        avg_train_acc = train_acc/train_data_size\n",
    " \n",
    "        avg_valid_loss = valid_loss/valid_data_size\n",
    "        avg_valid_acc = valid_acc/valid_data_size\n",
    "        \n",
    "        if avg_valid_loss > last_avg_valid_loss:\n",
    "            trigger_times += 1\n",
    "            print('trigger times:', trigger_times)\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                return model, history\n",
    "        \n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    " \n",
    "        if best_acc < avg_valid_acc:\n",
    "            best_acc = avg_valid_acc\n",
    "            best_epoch = epoch + 1\n",
    " \n",
    "        epoch_end = time.time()\n",
    " \n",
    "        print(\"Epoch: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation: Loss: {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(\n",
    "            epoch+1, avg_valid_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start\n",
    "        ))\n",
    "        print(\"Best Accuracy for validation : {:.4f} at epoch {:03d}\".format(best_acc, best_epoch))\n",
    " \n",
    "        torch.save(model, dataset+'_model_'+str(epoch+1)+'.pt')\n",
    "    \n",
    "        last_avg_valid_loss = avg_valid_loss\n",
    "        \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200\n",
      "Epoch: 001, Training: Loss: 0.9907, Accuracy: 65.4356%, \n",
      "\t\tValidation: Loss: 0.9907, Accuracy: 63.8095%, Time: 189.1098s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 2/200\n",
      "Epoch: 002, Training: Loss: 0.9478, Accuracy: 64.6780%, \n",
      "\t\tValidation: Loss: 0.9478, Accuracy: 62.8571%, Time: 187.1130s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 3/200\n",
      "trigger times: 1\n",
      "Epoch: 003, Training: Loss: 1.0642, Accuracy: 65.8144%, \n",
      "\t\tValidation: Loss: 1.0642, Accuracy: 59.0476%, Time: 190.6657s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 4/200\n",
      "Epoch: 004, Training: Loss: 0.9346, Accuracy: 62.7841%, \n",
      "\t\tValidation: Loss: 0.9346, Accuracy: 61.9048%, Time: 189.8512s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 5/200\n",
      "Epoch: 005, Training: Loss: 0.9096, Accuracy: 67.3295%, \n",
      "\t\tValidation: Loss: 0.9096, Accuracy: 56.1905%, Time: 198.8912s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 6/200\n",
      "trigger times: 2\n",
      "Epoch: 006, Training: Loss: 0.9134, Accuracy: 68.4659%, \n",
      "\t\tValidation: Loss: 0.9134, Accuracy: 61.9048%, Time: 194.2388s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 7/200\n",
      "trigger times: 3\n",
      "Epoch: 007, Training: Loss: 0.9570, Accuracy: 66.3826%, \n",
      "\t\tValidation: Loss: 0.9570, Accuracy: 60.9524%, Time: 197.9833s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 8/200\n",
      "Epoch: 008, Training: Loss: 0.9368, Accuracy: 68.8447%, \n",
      "\t\tValidation: Loss: 0.9368, Accuracy: 57.1429%, Time: 186.2685s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 9/200\n",
      "trigger times: 4\n",
      "Epoch: 009, Training: Loss: 1.0297, Accuracy: 65.3409%, \n",
      "\t\tValidation: Loss: 1.0297, Accuracy: 58.0952%, Time: 185.1619s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 10/200\n",
      "trigger times: 5\n",
      "Epoch: 010, Training: Loss: 1.0928, Accuracy: 63.4470%, \n",
      "\t\tValidation: Loss: 1.0928, Accuracy: 57.1429%, Time: 171.0834s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 11/200\n",
      "Epoch: 011, Training: Loss: 0.9995, Accuracy: 67.1402%, \n",
      "\t\tValidation: Loss: 0.9995, Accuracy: 59.0476%, Time: 177.6765s\n",
      "Best Accuracy for validation : 0.6381 at epoch 001\n",
      "Epoch: 12/200\n",
      "Epoch: 012, Training: Loss: 0.9360, Accuracy: 65.5303%, \n",
      "\t\tValidation: Loss: 0.9360, Accuracy: 64.7619%, Time: 174.8650s\n",
      "Best Accuracy for validation : 0.6476 at epoch 012\n",
      "Epoch: 13/200\n",
      "trigger times: 6\n",
      "Epoch: 013, Training: Loss: 0.9974, Accuracy: 64.8674%, \n",
      "\t\tValidation: Loss: 0.9974, Accuracy: 59.0476%, Time: 172.6854s\n",
      "Best Accuracy for validation : 0.6476 at epoch 012\n",
      "Epoch: 14/200\n",
      "trigger times: 7\n",
      "Epoch: 014, Training: Loss: 1.0683, Accuracy: 67.3295%, \n",
      "\t\tValidation: Loss: 1.0683, Accuracy: 60.0000%, Time: 180.8807s\n",
      "Best Accuracy for validation : 0.6476 at epoch 012\n",
      "Epoch: 15/200\n",
      "trigger times: 8\n",
      "Epoch: 015, Training: Loss: 1.0801, Accuracy: 67.6136%, \n",
      "\t\tValidation: Loss: 1.0801, Accuracy: 55.2381%, Time: 168.7914s\n",
      "Best Accuracy for validation : 0.6476 at epoch 012\n",
      "Epoch: 16/200\n",
      "Epoch: 016, Training: Loss: 0.9510, Accuracy: 63.5417%, \n",
      "\t\tValidation: Loss: 0.9510, Accuracy: 61.9048%, Time: 180.1155s\n",
      "Best Accuracy for validation : 0.6476 at epoch 012\n",
      "Epoch: 17/200\n",
      "trigger times: 9\n",
      "Epoch: 017, Training: Loss: 1.0575, Accuracy: 66.8561%, \n",
      "\t\tValidation: Loss: 1.0575, Accuracy: 58.0952%, Time: 168.0918s\n",
      "Best Accuracy for validation : 0.6476 at epoch 012\n",
      "Epoch: 18/200\n",
      "Epoch: 018, Training: Loss: 1.0180, Accuracy: 67.3295%, \n",
      "\t\tValidation: Loss: 1.0180, Accuracy: 60.0000%, Time: 182.1473s\n",
      "Best Accuracy for validation : 0.6476 at epoch 012\n",
      "Epoch: 19/200\n",
      "trigger times: 10\n",
      "Epoch: 019, Training: Loss: 1.0859, Accuracy: 69.4129%, \n",
      "\t\tValidation: Loss: 1.0859, Accuracy: 56.1905%, Time: 169.9149s\n",
      "Best Accuracy for validation : 0.6476 at epoch 012\n",
      "Epoch: 20/200\n",
      "Epoch: 020, Training: Loss: 1.0835, Accuracy: 67.3295%, \n",
      "\t\tValidation: Loss: 1.0835, Accuracy: 60.9524%, Time: 180.0961s\n",
      "Best Accuracy for validation : 0.6476 at epoch 012\n",
      "Epoch: 21/200\n",
      "Epoch: 021, Training: Loss: 0.9680, Accuracy: 67.2348%, \n",
      "\t\tValidation: Loss: 0.9680, Accuracy: 64.7619%, Time: 168.9122s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 22/200\n",
      "trigger times: 11\n",
      "Epoch: 022, Training: Loss: 0.9917, Accuracy: 67.7083%, \n",
      "\t\tValidation: Loss: 0.9917, Accuracy: 60.0000%, Time: 178.2192s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 23/200\n",
      "trigger times: 12\n",
      "Epoch: 023, Training: Loss: 1.0126, Accuracy: 67.1402%, \n",
      "\t\tValidation: Loss: 1.0126, Accuracy: 59.0476%, Time: 170.0199s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 24/200\n",
      "Epoch: 024, Training: Loss: 0.9171, Accuracy: 69.7917%, \n",
      "\t\tValidation: Loss: 0.9171, Accuracy: 57.1429%, Time: 173.9132s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 25/200\n",
      "trigger times: 13\n",
      "Epoch: 025, Training: Loss: 0.9366, Accuracy: 68.2765%, \n",
      "\t\tValidation: Loss: 0.9366, Accuracy: 59.0476%, Time: 174.7756s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 26/200\n",
      "trigger times: 14\n",
      "Epoch: 026, Training: Loss: 0.9482, Accuracy: 67.5189%, \n",
      "\t\tValidation: Loss: 0.9482, Accuracy: 62.8571%, Time: 167.8217s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 27/200\n",
      "trigger times: 15\n",
      "Epoch: 027, Training: Loss: 0.9654, Accuracy: 69.4129%, \n",
      "\t\tValidation: Loss: 0.9654, Accuracy: 60.9524%, Time: 180.3035s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 28/200\n",
      "trigger times: 16\n",
      "Epoch: 028, Training: Loss: 1.0289, Accuracy: 67.8977%, \n",
      "\t\tValidation: Loss: 1.0289, Accuracy: 60.0000%, Time: 168.1436s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 29/200\n",
      "Epoch: 029, Training: Loss: 1.0107, Accuracy: 70.6439%, \n",
      "\t\tValidation: Loss: 1.0107, Accuracy: 62.8571%, Time: 179.3330s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 30/200\n",
      "trigger times: 17\n",
      "Epoch: 030, Training: Loss: 1.1838, Accuracy: 69.3182%, \n",
      "\t\tValidation: Loss: 1.1838, Accuracy: 56.1905%, Time: 170.3831s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 31/200\n",
      "Epoch: 031, Training: Loss: 1.1617, Accuracy: 66.3826%, \n",
      "\t\tValidation: Loss: 1.1617, Accuracy: 53.3333%, Time: 175.4170s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 32/200\n",
      "Epoch: 032, Training: Loss: 1.0224, Accuracy: 67.7083%, \n",
      "\t\tValidation: Loss: 1.0224, Accuracy: 60.0000%, Time: 174.6425s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 33/200\n",
      "trigger times: 18\n",
      "Epoch: 033, Training: Loss: 1.0641, Accuracy: 66.6667%, \n",
      "\t\tValidation: Loss: 1.0641, Accuracy: 57.1429%, Time: 168.5451s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 34/200\n",
      "trigger times: 19\n",
      "Epoch: 034, Training: Loss: 1.1179, Accuracy: 68.2765%, \n",
      "\t\tValidation: Loss: 1.1179, Accuracy: 58.0952%, Time: 187.6326s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 35/200\n",
      "Epoch: 035, Training: Loss: 0.9797, Accuracy: 66.5720%, \n",
      "\t\tValidation: Loss: 0.9797, Accuracy: 61.9048%, Time: 175.5876s\n",
      "Best Accuracy for validation : 0.6476 at epoch 021\n",
      "Epoch: 36/200\n",
      "trigger times: 20\n",
      "Early stopping!\n",
      "Start to test process.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ResNet object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-27338612a4d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrained_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_history.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable ResNet object"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "trained_model, history = train_and_valid(resnet50, loss_func, optimizer, num_epochs)\n",
    "torch.save(history, dataset+'_history.pt')\n",
    "\n",
    "history = np.array(history)\n",
    "plt.plot(history[:, 0:2])\n",
    "plt.legend(['Tr Loss', 'Val Loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0, 1)\n",
    "plt.savefig(dataset+'_loss_curve.png')\n",
    "plt.show()\n",
    " \n",
    "plt.plot(history[:, 2:4])\n",
    "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.savefig(dataset+'_accuracy_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 63.01 %\n",
      "blazer acc = 44.44 %\n",
      "cardigan acc = 40.48 %\n",
      "coat acc = 62.79 %\n",
      "jacket acc = 84.62 %\n"
     ]
    }
   ],
   "source": [
    "class_acc = [0, 0, 0, 0]\n",
    "trained_model = torch.load('photos_model_33.pt')\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(resnet50.parameters())\n",
    "\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "for j, (inputs, labels) in enumerate(test_data):        \n",
    "    outputs = trained_model(inputs)\n",
    " \n",
    "    ret, predictions = torch.max(outputs.data, 1)\n",
    "    correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "    for l in range(len(predictions)):\n",
    "        if predictions[l] == labels[l]:\n",
    "            if labels[l].eq(0):\n",
    "                class_acc[0] +=1\n",
    "            elif labels[l].eq(1):\n",
    "                class_acc[1] +=1\n",
    "            elif labels[l].eq(2):\n",
    "                class_acc[2] +=1\n",
    "            else :\n",
    "                class_acc[3] +=1\n",
    "\n",
    "                \n",
    "    acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "    test_acc += acc.item() * inputs.size(0)\n",
    " \n",
    "    avg_test_acc = test_acc/test_data_size\n",
    "    \n",
    "        \n",
    "class_acc[0] = class_acc[0]/pic_num_test[0]\n",
    "class_acc[1] = class_acc[1]/pic_num_test[1]\n",
    "class_acc[2] = class_acc[2]/pic_num_test[2]\n",
    "class_acc[3] = class_acc[3]/pic_num_test[3]\n",
    "\n",
    "print(\"acc =\", \"%.2f\" %(avg_test_acc*100), \"%\")\n",
    "print(\"blazer acc =\", \"%.2f\" %(class_acc[0]*100), \"%\")\n",
    "print(\"cardigan acc =\", \"%.2f\" %(class_acc[1]*100), \"%\")\n",
    "print(\"coat acc =\", \"%.2f\" %(class_acc[2]*100), \"%\")\n",
    "print(\"jacket acc =\", \"%.2f\" %(class_acc[3]*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_2 = models.resnet50(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet50_2.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "fc_inputs = resnet50_2.fc.in_features\n",
    "\n",
    "resnet50.fc = nn.Sequential(\n",
    "    nn.Linear(fc_inputs, 4),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(resnet50_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7baa0d3eed8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrained_model2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_valid2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet50_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-20027752cdeb>\u001b[0m in \u001b[0;36mtrain_and_valid2\u001b[1;34m(model, loss_function, optimizer, epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1132\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1135\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "trained_model2, history = train_and_valid(resnet50_2, loss_func, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 63.70 %\n",
      "blazer acc = 55.56 %\n",
      "cardigan acc = 45.24 %\n",
      "coat acc = 48.84 %\n",
      "jacket acc = 92.31 %\n"
     ]
    }
   ],
   "source": [
    "class_acc = [0, 0, 0, 0]\n",
    "trained_model = torch.load('photos_model2_39.pt')\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(resnet50.parameters())\n",
    "\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "for j, (inputs, labels) in enumerate(test_data):        \n",
    "    outputs = trained_model(inputs)\n",
    " \n",
    "    ret, predictions = torch.max(outputs.data, 1)\n",
    "    correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "    for l in range(len(predictions)):\n",
    "        if predictions[l] == labels[l]:\n",
    "            if labels[l].eq(0):\n",
    "                class_acc[0] +=1\n",
    "            elif labels[l].eq(1):\n",
    "                class_acc[1] +=1\n",
    "            elif labels[l].eq(2):\n",
    "                class_acc[2] +=1\n",
    "            else :\n",
    "                class_acc[3] +=1\n",
    "\n",
    "                \n",
    "    acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "    test_acc += acc.item() * inputs.size(0)\n",
    " \n",
    "    avg_test_acc = test_acc/test_data_size\n",
    "    \n",
    "        \n",
    "class_acc[0] = class_acc[0]/pic_num_test[0]\n",
    "class_acc[1] = class_acc[1]/pic_num_test[1]\n",
    "class_acc[2] = class_acc[2]/pic_num_test[2]\n",
    "class_acc[3] = class_acc[3]/pic_num_test[3]\n",
    "\n",
    "print(\"acc =\", \"%.2f\" %(avg_test_acc*100), \"%\")\n",
    "print(\"blazer acc =\", \"%.2f\" %(class_acc[0]*100), \"%\")\n",
    "print(\"cardigan acc =\", \"%.2f\" %(class_acc[1]*100), \"%\")\n",
    "print(\"coat acc =\", \"%.2f\" %(class_acc[2]*100), \"%\")\n",
    "print(\"jacket acc =\", \"%.2f\" %(class_acc[3]*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我原本預期jacket會是第二，但預測jacket的準確率比我想像中高了很多，壓倒性的第一，而blazer則比我想像中低的多，我認為可能是因為電腦在影像辨識時，只看衣服佔圖片的長度，卻沒有辨識衣服在人身上的\"相對長度\"，再加上有些coat跟blazer看起來的材質相近，因此可能是因此常常把這兩者分類錯誤，才使得兩者的準確率都不高。\n",
    "而cardigan則是和預期的一樣，較難準確預測。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根據我的觀察，Q2的model的準確率比Q3的model好一些，我認為這是因為Q2的model會根據訓練狀況調整weight，提高效果較佳的layer的影響，使得在有early stopping的機制下，training時能夠抵達更深層，因此整體的預測效果會優於Q3在訓練過程中固定每層的weight的model，但Q2的model則需要花費更長的時間訓練。而Q4的model由於未使用預初始化好的模型，因此可能因未初始化或初始化不當導致梯度收斂狀況較差，因此結果不一定比Q2好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
